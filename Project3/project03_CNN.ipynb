{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project03_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fBlEGazCut3",
        "outputId": "f77bc83b-6d7b-4d14-93b5-c314ded1c23f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive2',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSldfrtNDUHM",
        "outputId": "4afda421-f39b-4c90-9b02-5108639b2fa5"
      },
      "source": [
        "# install stop_words if not installed\r\n",
        "!pip install stop_words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32919 sha256=713f155008d2e3a395aa92123bde0ce7c5682d1f2906fcceab9b3a71c06b014f\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwfDBrBgE331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d34d99c3-695f-42ea-a872-a3078c2885f1"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import codecs\r\n",
        "from sklearn import preprocessing\r\n",
        "from stop_words import get_stop_words # could be deleted\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction import text\r\n",
        "import sklearn.model_selection\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "import re\r\n",
        "import time\r\n",
        "import scipy\r\n",
        "import tensorflow as tf\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mab_9c2lRCRD"
      },
      "source": [
        "import keras\r\n",
        "from keras.preprocessing.text import one_hot,Tokenizer\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzDK6avFE55Q"
      },
      "source": [
        "global all_stop_words\r\n",
        "all_stop_words = get_stop_words('turkish')\r\n",
        "additional_stopwords = ['acaba','ama','aslında','az','bazı','belki','biri','birkaç','birşey','biz','bu','çok','çünkü','da','daha','de','defa','diğer','eğer','en','gibi','hem','hepsi','her','hiç','için','ile','ise','kez','ki','kim','mi','mü','nasıl','ne','neden','nerde','nerede','nereye','niçin','niye','o','sanki','şey','şu','siz','tüm','ve','veya','ya','yani','nin','ın','in','nın','nde','den','dan','nda']\r\n",
        "all_stop_words += additional_stopwords"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE4IcI1lE_MI"
      },
      "source": [
        "train_data = pd.read_csv(\"/content/drive2/MyDrive/NLP/Dataset/train.csv\")\r\n",
        "test_data = pd.read_csv(\"/content/drive2/MyDrive/NLP/Dataset/test.csv\")"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CaSSIQJFATL"
      },
      "source": [
        "def preprocessText(text):\r\n",
        "  text = text.lower()\r\n",
        "  word_list = text.split()\r\n",
        "  for word in word_list:\r\n",
        "    if word in all_stop_words:\r\n",
        "      word_list.remove(word)\r\n",
        "  text = \" \".join(word_list)\r\n",
        "  return text"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zgVwaTcFCR1"
      },
      "source": [
        "train_data['text'] = train_data['text'].apply(lambda x: preprocessText(x))\r\n",
        "test_data['text'] = test_data['text'].apply(lambda x: preprocessText(x))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5vKI4TzFFoc"
      },
      "source": [
        "train_Data, validation_Data = sklearn.model_selection.train_test_split(train_data, train_size = 0.8, random_state=0)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL-EsW2VQo5E"
      },
      "source": [
        "types = train_data.label.unique()\r\n",
        "label_dict={}\r\n",
        "for i,text_label in enumerate(types):\r\n",
        "    label_dict[text_label]=i\r\n",
        "labels=train_data.label.apply(lambda x: label_dict[x])"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP57dwYolCdz"
      },
      "source": [
        "types = test_data.label.unique()\r\n",
        "label_dict={}\r\n",
        "for i,text_label in enumerate(types):\r\n",
        "    label_dict[text_label]=i\r\n",
        "test_labels=test_data.label.apply(lambda x:label_dict[x])"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-AJSKwNP4Gf",
        "outputId": "47bbdea0-d6bc-4491-8ad2-2c4af84dfc64"
      },
      "source": [
        "# another approach (working one)\r\n",
        "texts=train_Data.text\r\n",
        "vocab_size=1000000 # guarantee to consider all the vocabulaires\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size,filters='!\"$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\r\n",
        "                      lower=True)\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "sequences_train = tokenizer.texts_to_sequences(texts)\r\n",
        "sequences_valid=tokenizer.texts_to_sequences(validation_Data.text)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 166525 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UosDO9eaOX7i",
        "outputId": "9d4559cb-d85d-450d-b692-d0b20c7e7005"
      },
      "source": [
        "#download it if required\r\n",
        "nltk.download('punkt') "
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4nXgWWaI7Mr"
      },
      "source": [
        "# calculate the maximum length sequences in the train,validation and test data (these operations may take a few seconds)\r\n",
        "max_length_train =- 1\r\n",
        "for doc in train_Data['text']:\r\n",
        "    tokens=nltk.word_tokenize(doc)\r\n",
        "    if (max_length_train < len(tokens)):\r\n",
        "        max_length_train = len(tokens)\r\n",
        "\r\n",
        "max_length_validation =- 1\r\n",
        "for doc in validation_Data['text']:\r\n",
        "    tokens=nltk.word_tokenize(doc)\r\n",
        "    if (max_length_validation < len(tokens)):\r\n",
        "        max_length_validation = len(tokens)\r\n",
        "\r\n",
        "max_length_test =- 1\r\n",
        "for doc in test_data['text']:\r\n",
        "    tokens=nltk.word_tokenize(doc)\r\n",
        "    if (max_length_test < len(tokens)):\r\n",
        "        max_length_test = len(tokens)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sSkLk6zQMBP",
        "outputId": "71aa6cef-a399-4631-ef0b-7021ff6cb74b"
      },
      "source": [
        "# make sure to have all the data sequences have the same length\r\n",
        "X_train  = pad_sequences(sequences_train,maxlen = max(max_length_train,max_length_validation))\r\n",
        "X_val    = pad_sequences(sequences_valid,maxlen = max(max_length_train,max_length_validation))\r\n",
        "y_train  = train_Data['label']\r\n",
        "y_val    = validation_Data['label']\r\n",
        "label_encoder = preprocessing.LabelEncoder()\r\n",
        "y_train = label_encoder.fit_transform(y_train)\r\n",
        "y_val = label_encoder.fit_transform(y_val)\r\n",
        "#y_train = to_categorical(np.asarray(labels[train_Data.index]))\r\n",
        "#y_val = to_categorical(np.asarray(labels[validation_Data.index]))\r\n",
        "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\r\n",
        "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train and X validation tensor: (6400, 7513) (1600, 7513)\n",
            "Shape of label train and validation tensor: (6400,) (1600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0E3j-wr9BMz"
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU71y2Ucnvcj"
      },
      "source": [
        "# load pretrained model\r\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive2/MyDrive/NLP/trmodel', binary=True)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiI_ttpWC4KS"
      },
      "source": [
        "# load pretrained model\r\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive2/MyDrive/NLP/my_model.bin', binary=True) # with 5k dataset"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWAdqn3HgVQT"
      },
      "source": [
        "#my pretrained model\r\n",
        "word_vectors = KeyedVectors.load_word2vec_format('/content/drive2/MyDrive/NLP/my_model_100k.bin', binary=True) # with 100k dataset"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrE0d6Rn7hB4"
      },
      "source": [
        "# without pretrained model\r\n",
        "embedding_dim=400\r\n",
        "vocabulary_size=min(len(word_index)+1,vocab_size)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyprtRCxfYOB"
      },
      "source": [
        "embedding_dim    = 100 # it was 400 for the large pretrained model\r\n",
        "vocabulary_size  = min(len(word_index)+1,vocab_size)\r\n",
        "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\r\n",
        "for word, i in word_index.items():\r\n",
        "    try:\r\n",
        "      embedding_vector    = word_vectors[word]\r\n",
        "      embedding_matrix[i] = embedding_vector\r\n",
        "    except KeyError:\r\n",
        "      #print(\"key error\")\r\n",
        "      embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTS_tUNouCC7"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (1conv1d layer)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,trainable = True,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPxLxZMr4PHD"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (1conv1d layer) static CNN\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,trainable = False,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlKBSku_x9xx"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (2conv1d layer)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,trainable = True,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBH04ZaY7Rnf"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (2conv1d layer)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,trainable = False,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEOOiZtUyNM8"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (3conv1d layer)\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PND07Io80FR"
      },
      "source": [
        "# basic cnn architecture without pretrained embedding (3conv1d layer) CNN Static\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,trainable = False,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLXroArqBZvl"
      },
      "source": [
        "# basic cnn architecture with pretraining\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,weights=[embedding_matrix],trainable=True,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnabLcOl_3et"
      },
      "source": [
        "# basic cnn architecture with pretraining CNN Static\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocabulary_size,embedding_dim,weights=[embedding_matrix],trainable=False,input_length = max(max_length_train,max_length_validation)))\r\n",
        "model.add(Conv1D(100,2, padding='same', activation='relu'))\r\n",
        "model.add(MaxPooling1D())\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LznkyuhSvf_"
      },
      "source": [
        "# more complex cnn architecture from kaggle notebook\r\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\r\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\r\n",
        "from keras.layers.core import Reshape, Flatten\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.models import Model\r\n",
        "from keras import regularizers\r\n",
        "sequence_length = X_train.shape[1]\r\n",
        "filter_sizes = [3,4,5]\r\n",
        "num_filters = 100\r\n",
        "drop = 0.5\r\n",
        "\r\n",
        "pretrained = 1\r\n",
        "if pretrained:\r\n",
        "  embedding_layer = Embedding(vocabulary_size,\r\n",
        "                            embedding_dim,\r\n",
        "                            weights=[embedding_matrix],\r\n",
        "                            trainable=True)\r\n",
        "else:\r\n",
        "  embedding_layer = Embedding(vocabulary_size,\r\n",
        "                            embedding_dim)\r\n",
        "\r\n",
        "inputs = Input(shape=(sequence_length,))\r\n",
        "embedding = embedding_layer(inputs)\r\n",
        "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\r\n",
        "\r\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\r\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\r\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\r\n",
        "\r\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\r\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\r\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\r\n",
        "\r\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\r\n",
        "#print(merged_tensor.shape)\r\n",
        "flatten = Flatten()(merged_tensor)\r\n",
        "#print(flatten.shape)\r\n",
        "reshape = Reshape((3*num_filters,))(flatten)\r\n",
        "\r\n",
        "dropout = Dropout(drop)(flatten)\r\n",
        "output = Dense(units=5, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\r\n",
        "\r\n",
        "# this creates a model that includes\r\n",
        "model = Model(inputs, output)\r\n",
        "#inputs = Input(shape=(sequence_length,))\r\n",
        "#embedding = embedding_layer(inputs)\r\n",
        "#reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\r\n",
        "#output = Dense(units=5, activation='softmax')\r\n",
        "\r\n",
        "# this creates a model that includes\r\n",
        "#model = Model(inputs, output)\r\n",
        "\r\n",
        "# Reference:\r\n",
        "#https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBLSqJC6f6LP",
        "outputId": "5f00b2d5-5760-4d4e-83ed-8890ab3580f7"
      },
      "source": [
        "#optimizer = Adam(lr=0.0001,decay=0.0, amsgrad=True)\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
        "model.summary()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 7513, 100)         16652600  \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 7513, 100)         20100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling (None, 3756, 100)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 375600)            0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               192307712 \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 208,982,977\n",
            "Trainable params: 192,330,377\n",
            "Non-trainable params: 16,652,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8WqH5IEufpf"
      },
      "source": [
        "sequences_test=tokenizer.texts_to_sequences(test_data.text)\r\n",
        "X_test = pad_sequences(sequences_test,maxlen=max(max_length_train,max_length_validation))\r\n",
        "y_test = test_data['label']\r\n",
        "y_test = label_encoder.fit_transform(y_test)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTIDkm4U4dfa",
        "outputId": "279e99f6-93c4-4251-d502-12803f5b1db8"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (1 convd layer) CNN static\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 47s 231ms/step - loss: 2.2244 - acc: 0.4456 - val_loss: 1.0013 - val_acc: 0.6081\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 46s 231ms/step - loss: 0.7637 - acc: 0.7211 - val_loss: 0.7519 - val_acc: 0.7125\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 46s 231ms/step - loss: 0.3712 - acc: 0.8918 - val_loss: 0.7140 - val_acc: 0.7231\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 46s 232ms/step - loss: 0.1851 - acc: 0.9592 - val_loss: 0.6931 - val_acc: 0.7556\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.6931 - acc: 0.7556\n",
            "Validation Accuracy: 75.56%\n",
            "63/63 [==============================] - 9s 135ms/step - loss: 0.6874 - acc: 0.7555\n",
            "Test Accuracy: 0.7555000185966492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNesqhiZuRa6",
        "outputId": "859c96f6-2ac2-45bf-bae3-a3807ac25677"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (1 convd layer)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 184s 883ms/step - loss: 2.1094 - acc: 0.5265 - val_loss: 0.4512 - val_acc: 0.8369\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 176s 879ms/step - loss: 0.1687 - acc: 0.9481 - val_loss: 0.5092 - val_acc: 0.8281\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 176s 878ms/step - loss: 0.0113 - acc: 0.9979 - val_loss: 0.5544 - val_acc: 0.8369\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 175s 874ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.5583 - val_acc: 0.8450\n",
            "50/50 [==============================] - 6s 115ms/step - loss: 0.5583 - acc: 0.8450\n",
            "Accuracy: 84.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPpcbU5_xc3P",
        "outputId": "ea58a9a7-fedc-4794-9453-d264c262a1e2"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (2 convd layer)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 175s 870ms/step - loss: 1.2571 - acc: 0.4849 - val_loss: 0.5222 - val_acc: 0.8094\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 173s 866ms/step - loss: 0.2610 - acc: 0.9094 - val_loss: 0.4799 - val_acc: 0.8356\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 173s 864ms/step - loss: 0.0227 - acc: 0.9926 - val_loss: 0.6454 - val_acc: 0.8394\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 173s 867ms/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.6730 - val_acc: 0.8531\n",
            "50/50 [==============================] - 6s 118ms/step - loss: 0.6730 - acc: 0.8531\n",
            "Validation Accuracy: 85.31%\n",
            "63/63 [==============================] - 8s 118ms/step - loss: 0.7600 - acc: 0.8285\n",
            "Test Accuracy: 0.828499972820282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrE5WEK77oEu",
        "outputId": "73cb7950-1b78-4b0d-de1a-ab3e7eb9e24f"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (2 convd layer) CNN static\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 46s 227ms/step - loss: 1.5722 - acc: 0.4114 - val_loss: 1.1091 - val_acc: 0.5069\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 1.0196 - acc: 0.5567 - val_loss: 0.8301 - val_acc: 0.6562\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.6580 - acc: 0.7353 - val_loss: 0.7121 - val_acc: 0.7306\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 45s 227ms/step - loss: 0.4103 - acc: 0.8506 - val_loss: 0.6406 - val_acc: 0.7681\n",
            "50/50 [==============================] - 7s 139ms/step - loss: 0.6406 - acc: 0.7681\n",
            "Validation Accuracy: 76.81%\n",
            "63/63 [==============================] - 9s 138ms/step - loss: 0.6381 - acc: 0.7675\n",
            "Test Accuracy: 0.7674999833106995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcFqrW3eyTu-",
        "outputId": "11b18fa8-aa5c-46a4-dafb-35743721559d"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (3 convd layer)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 175s 871ms/step - loss: 1.2733 - acc: 0.4669 - val_loss: 0.5768 - val_acc: 0.7825\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 173s 867ms/step - loss: 0.3444 - acc: 0.8809 - val_loss: 0.5386 - val_acc: 0.8169\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 173s 866ms/step - loss: 0.0555 - acc: 0.9831 - val_loss: 1.0221 - val_acc: 0.7681\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 174s 869ms/step - loss: 0.0433 - acc: 0.9898 - val_loss: 1.0593 - val_acc: 0.7894\n",
            "50/50 [==============================] - 7s 130ms/step - loss: 1.0593 - acc: 0.7894\n",
            "Validation Accuracy: 78.94%\n",
            "63/63 [==============================] - 8s 130ms/step - loss: 1.0100 - acc: 0.7915\n",
            "Test Accuracy: 0.7914999723434448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwIcVAeB882I",
        "outputId": "9a17778a-dc9c-4bef-b7dd-74bd554a64fa"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding (3 convd layer) CNN static\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 45s 223ms/step - loss: 1.4258 - acc: 0.3893 - val_loss: 1.1428 - val_acc: 0.4737\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 44s 222ms/step - loss: 1.0630 - acc: 0.5250 - val_loss: 0.7998 - val_acc: 0.6744\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 45s 223ms/step - loss: 0.6490 - acc: 0.7379 - val_loss: 0.6576 - val_acc: 0.7650\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 45s 223ms/step - loss: 0.4281 - acc: 0.8352 - val_loss: 0.6121 - val_acc: 0.7800\n",
            "50/50 [==============================] - 7s 140ms/step - loss: 0.6121 - acc: 0.7800\n",
            "Validation Accuracy: 78.00%\n",
            "63/63 [==============================] - 9s 140ms/step - loss: 0.6435 - acc: 0.7770\n",
            "Test Accuracy: 0.7770000100135803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oQ2baZc7HMJ",
        "outputId": "a67495f1-ae5e-4d22-d309-2db3cba951df"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on large Turkish pretrained model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 175s 874ms/step - loss: 3.2832 - acc: 0.6163 - val_loss: 0.5360 - val_acc: 0.8213\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 175s 875ms/step - loss: 0.2240 - acc: 0.9304 - val_loss: 0.4711 - val_acc: 0.8306\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 169s 843ms/step - loss: 0.0447 - acc: 0.9921 - val_loss: 0.6008 - val_acc: 0.8281\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 168s 838ms/step - loss: 0.0075 - acc: 0.9990 - val_loss: 0.6682 - val_acc: 0.8369\n",
            "50/50 [==============================] - 6s 126ms/step - loss: 0.6682 - acc: 0.8369\n",
            "Validation Accuracy: 83.69%\n",
            "63/63 [==============================] - 8s 125ms/step - loss: 0.7188 - acc: 0.8315\n",
            "Test Accuracy: 0.8314999938011169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iYZgZ7z-hZG",
        "outputId": "ffb99639-d6d5-4945-f61e-335da22a6e72"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on large Turkish pretrained model (CNN Static)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 45s 220ms/step - loss: 3.1740 - acc: 0.5988 - val_loss: 0.5367 - val_acc: 0.8225\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 44s 219ms/step - loss: 0.2394 - acc: 0.9231 - val_loss: 0.5345 - val_acc: 0.8169\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 44s 219ms/step - loss: 0.0727 - acc: 0.9824 - val_loss: 0.6311 - val_acc: 0.8175\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 44s 218ms/step - loss: 0.0112 - acc: 0.9998 - val_loss: 0.6783 - val_acc: 0.8294\n",
            "50/50 [==============================] - 6s 124ms/step - loss: 0.6783 - acc: 0.8294\n",
            "Validation Accuracy: 82.94%\n",
            "63/63 [==============================] - 8s 124ms/step - loss: 0.7190 - acc: 0.8265\n",
            "Test Accuracy: 0.8264999985694885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOHp0plz_Vdk",
        "outputId": "08cbbde2-1db6-4334-bc7c-34a4c3b8dc28"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on my 100k word2vec model in project 2\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 136s 678ms/step - loss: 1.5909 - acc: 0.5973 - val_loss: 0.4722 - val_acc: 0.8456\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 135s 673ms/step - loss: 0.1729 - acc: 0.9467 - val_loss: 0.6332 - val_acc: 0.8006\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 134s 672ms/step - loss: 0.0241 - acc: 0.9959 - val_loss: 0.5211 - val_acc: 0.8519\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 134s 668ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.5888 - val_acc: 0.8469\n",
            "50/50 [==============================] - 4s 90ms/step - loss: 0.5888 - acc: 0.8469\n",
            "Validation Accuracy: 84.69%\n",
            "63/63 [==============================] - 6s 90ms/step - loss: 0.6057 - acc: 0.8365\n",
            "Test Accuracy: 0.8364999890327454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svM-BrOt__Tv",
        "outputId": "d5d565cc-c5b4-4772-b90e-bc701116d641"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on my 100k word2vec model in project 2 (CNN Static)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 1.9691 - acc: 0.5433 - val_loss: 0.5116 - val_acc: 0.8200\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.2619 - acc: 0.9131 - val_loss: 0.4665 - val_acc: 0.8344\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.0765 - acc: 0.9859 - val_loss: 0.5227 - val_acc: 0.8363\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 38s 189ms/step - loss: 0.0131 - acc: 0.9997 - val_loss: 0.5936 - val_acc: 0.8419\n",
            "50/50 [==============================] - 5s 104ms/step - loss: 0.5936 - acc: 0.8419\n",
            "Validation Accuracy: 84.19%\n",
            "63/63 [==============================] - 7s 105ms/step - loss: 0.6489 - acc: 0.8335\n",
            "Test Accuracy: 0.8335000276565552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8bJU93zDIfU",
        "outputId": "cb19048a-671b-4c9c-a8fe-d31990b84314"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on my 5k   word2vec model in project 2\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) \r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 55s 272ms/step - loss: 1.8825 - acc: 0.5615 - val_loss: 0.5182 - val_acc: 0.8050\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 54s 271ms/step - loss: 0.2708 - acc: 0.9004 - val_loss: 0.4653 - val_acc: 0.8275\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 56s 278ms/step - loss: 0.0749 - acc: 0.9856 - val_loss: 0.5799 - val_acc: 0.8100\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 54s 270ms/step - loss: 0.0082 - acc: 0.9998 - val_loss: 0.5445 - val_acc: 0.8381\n",
            "50/50 [==============================] - 2s 37ms/step - loss: 0.5445 - acc: 0.8381\n",
            "Validation Accuracy: 83.81%\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.6131 - acc: 0.8215\n",
            "Test Accuracy: 0.8215000033378601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUHGibo4R4oD",
        "outputId": "5ccb3d31-cb82-46c3-da08-054b76066964"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 64,shuffle = True) # train without pretrained model\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "100/100 [==============================] - 4201s 42s/step - loss: 1.5927 - acc: 0.4370 - val_loss: 1.0200 - val_acc: 0.7719\n",
            "Epoch 2/4\n",
            "100/100 [==============================] - 4216s 42s/step - loss: 0.9447 - acc: 0.7838 - val_loss: 0.8573 - val_acc: 0.7981\n",
            "Epoch 3/4\n",
            "100/100 [==============================] - 4296s 43s/step - loss: 0.7749 - acc: 0.8443 - val_loss: 0.7953 - val_acc: 0.8163\n",
            "Epoch 4/4\n",
            "100/100 [==============================] - 4329s 43s/step - loss: 0.6505 - acc: 0.9020 - val_loss: 0.7923 - val_acc: 0.8131\n",
            "50/50 [==============================] - 234s 5s/step - loss: 0.7923 - acc: 0.8131\n",
            "Accuracy: 81.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4MuwoRegvMd",
        "outputId": "2fdce63e-0be5-4f68-ef1e-ca8b20e87118"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) # train with my 5k pretrained model\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 1169s 6s/step - loss: 1.4051 - acc: 0.5233 - val_loss: 1.0788 - val_acc: 0.6963\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 1171s 6s/step - loss: 0.9756 - acc: 0.7433 - val_loss: 0.9156 - val_acc: 0.7600\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 1173s 6s/step - loss: 0.8093 - acc: 0.8289 - val_loss: 0.8814 - val_acc: 0.7663\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 1175s 6s/step - loss: 0.6884 - acc: 0.8981 - val_loss: 0.8937 - val_acc: 0.7900\n",
            "50/50 [==============================] - 59s 1s/step - loss: 0.8937 - acc: 0.7900\n",
            "Accuracy: 79.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8kEWHO-Cpu6",
        "outputId": "aaf6c571-8680-46c4-e9d1-17c768a63d78"
      },
      "source": [
        "# train with basic cnn layer without pretrained embedding on my 5k word2vec model in project 2 (CNN Static)\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) # train with my 5k pretrained model\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 21s 101ms/step - loss: 1.9737 - acc: 0.5581 - val_loss: 0.5885 - val_acc: 0.7800\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 20s 101ms/step - loss: 0.3354 - acc: 0.8838 - val_loss: 0.6089 - val_acc: 0.7987\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 20s 101ms/step - loss: 0.1184 - acc: 0.9745 - val_loss: 0.6393 - val_acc: 0.7875\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 20s 101ms/step - loss: 0.0261 - acc: 0.9974 - val_loss: 0.7009 - val_acc: 0.7981\n",
            "50/50 [==============================] - 2s 40ms/step - loss: 0.7009 - acc: 0.7981\n",
            "Validation Accuracy: 79.81%\n",
            "63/63 [==============================] - 3s 41ms/step - loss: 0.6862 - acc: 0.8040\n",
            "Test Accuracy: 0.8040000200271606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI8XumEbatnN",
        "outputId": "dafebd54-b562-4ed4-9383-674eb9899005"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) # train with my 100k pretrained model\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 161s 765ms/step - loss: 1.6377 - acc: 0.4203 - val_loss: 1.1997 - val_acc: 0.7588\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 150s 749ms/step - loss: 1.1289 - acc: 0.7983 - val_loss: 1.0773 - val_acc: 0.8062\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 150s 749ms/step - loss: 0.9673 - acc: 0.8617 - val_loss: 1.0433 - val_acc: 0.8044\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 149s 746ms/step - loss: 0.8244 - acc: 0.9082 - val_loss: 0.9811 - val_acc: 0.8219\n",
            "50/50 [==============================] - 7s 134ms/step - loss: 0.9811 - acc: 0.8219\n",
            "Accuracy: 82.19%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMpDGfHTpCWI",
        "outputId": "e355cc82-fdee-4e2f-f65c-2b93de5d4af6"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) # train with my 100k with more complex kaggle CNN architecture\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))x\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 181s 895ms/step - loss: 1.2300 - acc: 0.6500 - val_loss: 0.9116 - val_acc: 0.8356\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 178s 892ms/step - loss: 0.8598 - acc: 0.8636 - val_loss: 0.8339 - val_acc: 0.8550\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 179s 895ms/step - loss: 0.7172 - acc: 0.9061 - val_loss: 0.8112 - val_acc: 0.8512\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 177s 887ms/step - loss: 0.6331 - acc: 0.9381 - val_loss: 0.7885 - val_acc: 0.8544\n",
            "50/50 [==============================] - 10s 191ms/step - loss: 0.7885 - acc: 0.8544\n",
            "Accuracy: 85.44%\n",
            "63/63 [==============================] - 12s 195ms/step - loss: 0.7985 - acc: 0.8450\n",
            "Test Accuracy: 0.8450000286102295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk-tMhb4tJ3r",
        "outputId": "cda5f5d2-3745-4619-c124-d22466bd6715"
      },
      "source": [
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 32,shuffle = True) # train with trmodel with more complex kaggle CNN architecture\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Validation Accuracy: %.2f%%\" % (scores[1]*100))\r\n",
        "print(\"Test Accuracy:\", model.evaluate(X_test,y_test)[1])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "200/200 [==============================] - 221s 1s/step - loss: 1.4157 - acc: 0.6345 - val_loss: 1.0459 - val_acc: 0.8400\n",
            "Epoch 2/4\n",
            "200/200 [==============================] - 217s 1s/step - loss: 1.1093 - acc: 0.8122 - val_loss: 1.1032 - val_acc: 0.8169\n",
            "Epoch 3/4\n",
            "200/200 [==============================] - 218s 1s/step - loss: 0.9791 - acc: 0.8535 - val_loss: 0.9814 - val_acc: 0.8363\n",
            "Epoch 4/4\n",
            "200/200 [==============================] - 222s 1s/step - loss: 0.8858 - acc: 0.8710 - val_loss: 0.9085 - val_acc: 0.8656\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.9085 - acc: 0.8656\n",
            "Validation Accuracy: 86.56%\n",
            "63/63 [==============================] - 16s 244ms/step - loss: 0.9326 - acc: 0.8410\n",
            "Test Accuracy: 0.8410000205039978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_ToTJ6rDNlc",
        "outputId": "8f09f4e6-3c09-4151-c124-79534b582abe"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 64) # train with simpler model\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "100/100 [==============================] - 61s 605ms/step - loss: 3.2821 - acc: 0.5511 - val_loss: 0.5358 - val_acc: 0.8169\n",
            "Epoch 2/4\n",
            "100/100 [==============================] - 60s 604ms/step - loss: 0.3087 - acc: 0.9000 - val_loss: 0.4706 - val_acc: 0.8313\n",
            "Epoch 3/4\n",
            "100/100 [==============================] - 61s 606ms/step - loss: 0.0873 - acc: 0.9761 - val_loss: 0.5452 - val_acc: 0.8200\n",
            "Epoch 4/4\n",
            "100/100 [==============================] - 61s 607ms/step - loss: 0.0198 - acc: 0.9996 - val_loss: 0.6272 - val_acc: 0.8344\n",
            "50/50 [==============================] - 9s 171ms/step - loss: 0.6272 - acc: 0.8344\n",
            "Accuracy: 83.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80_i021INv9L",
        "outputId": "9980c2b6-b67e-4147-e08f-4680efca5c4e"
      },
      "source": [
        "# train with kaggle approach\r\n",
        "#Fitting the data onto model\r\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=4,batch_size = 64) # train with label encoder version\r\n",
        "# Getting score metrics from our model\r\n",
        "scores = model.evaluate(X_val, y_val)\r\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "100/100 [==============================] - 61s 606ms/step - loss: 3.4908 - acc: 0.5325 - val_loss: 0.5312 - val_acc: 0.8081\n",
            "Epoch 2/4\n",
            "100/100 [==============================] - 60s 604ms/step - loss: 0.3217 - acc: 0.8914 - val_loss: 0.4870 - val_acc: 0.8275\n",
            "Epoch 3/4\n",
            "100/100 [==============================] - 60s 605ms/step - loss: 0.1248 - acc: 0.9647 - val_loss: 0.5764 - val_acc: 0.8219\n",
            "Epoch 4/4\n",
            "100/100 [==============================] - 61s 611ms/step - loss: 0.0535 - acc: 0.9943 - val_loss: 0.6113 - val_acc: 0.8300\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.6113 - acc: 0.8300\n",
            "Accuracy: 83.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}