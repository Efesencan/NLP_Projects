{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JlFM_WaSC02",
        "outputId": "1a90e24e-30e8-482a-c590-7ec8f76e4ca1"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive2',force_remount=True)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4naxSnGrp5Y"
      },
      "source": [
        "# read gazeteers\r\n",
        "organizations = {}\r\n",
        "locations = {}\r\n",
        "persons = {}\r\n",
        "organizational_gazeteer = open(\"/content/drive2/MyDrive/NLP/organizational_gazeteer.txt\",\"r\")\r\n",
        "location_gazeteer = open(\"/content/drive2/MyDrive/NLP/location_gazeteer.txt\",\"r\")\r\n",
        "person_gazeteer = open(\"/content/drive2/MyDrive/NLP/person_gazeteer.txt\",\"r\")"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYyV6t7lKVAx"
      },
      "source": [
        "# create organization lists\r\n",
        "for line in organizational_gazeteer:\r\n",
        "  line = line.strip()\r\n",
        "  organizations[line] = 1\r\n",
        "# create location lists\r\n",
        "for line in location_gazeteer:\r\n",
        "  line = line.strip()\r\n",
        "  locations[line] = 1\r\n",
        "\r\n",
        "# create person lists\r\n",
        "for line in person_gazeteer:\r\n",
        "  line = line.strip()\r\n",
        "  persons[line] = 1"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpRxy0PDxfnv",
        "outputId": "e07d5c08-5ce7-4242-85ff-18df80111a69"
      },
      "source": [
        "# install libraries\r\n",
        "!pip install sklearn_crfsuite\r\n",
        "!pip install eli5"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.7)\n",
            "Requirement already satisfied: eli5 in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.4.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.11.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.15.0)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (20.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->eli5) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhgtvC6zsPZm"
      },
      "source": [
        "import sklearn_crfsuite\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "import eli5\r\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.metrics import plot_confusion_matrix\r\n",
        "from sklearn_crfsuite import scorers\r\n",
        "from sklearn_crfsuite import metrics\r\n",
        "from collections import Counter"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB-hVHdAy0qh"
      },
      "source": [
        "all_sents = []\r\n",
        "count = 0\r\n",
        "with open(\"/content/drive2/MyDrive/NLP/NE.txt\",\"r\") as ner:\r\n",
        "  for line in ner: # reading the file line by line\r\n",
        "    train_sent = []\r\n",
        "    count = 0\r\n",
        "    #line = line.replace(\" '\",\"'\")\r\n",
        "    #line = line.replace(\"' \",\"'\")\r\n",
        "    #line = line.replace(\"  \",\" \")\r\n",
        "    ner_type = \"\"\r\n",
        "    b_enamex = 0\r\n",
        "    start = 0\r\n",
        "    end = 0\r\n",
        "    for word in line.split():\r\n",
        "      #print(word)\r\n",
        "      if word == \"<b_enamex\": # this means we will encounter a word with tags other than 'O'\r\n",
        "        b_enamex = 1\r\n",
        "        start = 1\r\n",
        "      elif \"e_enamex\" in word:\r\n",
        "        b_enamex = 0\r\n",
        "        if \"TYPE\" in word:\r\n",
        "          match = re.findall('=\"([A-ZÇĞİÖŞÜ]*[a-zçğıöşü]*)\"',word)\r\n",
        "          if len(match):\r\n",
        "            match = match[0]\r\n",
        "            #ner_type = match\r\n",
        "            if start:\r\n",
        "                ner_type = \"B-\"\r\n",
        "                start = 0\r\n",
        "            else:\r\n",
        "              ner_type = \"I-\"\r\n",
        "            if match == \"ORGANIZATION\":\r\n",
        "              ner_type += \"ORG\"\r\n",
        "            if match == \"LOCATION\":\r\n",
        "              ner_type += \"LOC\"\r\n",
        "            if match == \"PERSON\":\r\n",
        "              ner_type += \"PER\"\r\n",
        "            \r\n",
        "            word = re.findall('>([A-ZÇĞİÖŞÜ]*[a-zçğıöşü]*)\\s*',word)\r\n",
        "            word =word[0]\r\n",
        "            #print(\"-----\",word,ner_type)\r\n",
        "            real_ner = \"B\" # it was 'U'\r\n",
        "            real_ner += ner_type[1:]\r\n",
        "            ner_type = real_ner\r\n",
        "            train_sent.append([word,ner_type])\r\n",
        "            count += 1\r\n",
        "          #print(\"ENAMEX TYPE:\",match)\r\n",
        "        else:\r\n",
        "          if start:\r\n",
        "            ner_type = \"B-\"\r\n",
        "            start = 0\r\n",
        "          else:\r\n",
        "            ner_type = \"I-\"\r\n",
        "          if match == \"ORGANIZATION\":\r\n",
        "            ner_type += \"ORG\"\r\n",
        "          if match == \"LOCATION\":\r\n",
        "            ner_type += \"LOC\"\r\n",
        "          if match == \"PERSON\":\r\n",
        "            ner_type += \"PER\"\r\n",
        "          word = re.findall('>?([A-ZÇĞİÖŞÜ]*[a-zçğıöşü]*)<e_enamex>',word)\r\n",
        "          if len(word):\r\n",
        "            word = word[0]\r\n",
        "            train_sent.append([word,ner_type])\r\n",
        "            count += 1\r\n",
        "      else:\r\n",
        "        if b_enamex:\r\n",
        "          if \"TYPE\" in word:\r\n",
        "            match = re.findall('=\"([A-ZÇĞİÖŞÜ]*[a-zçğıöşü]*)\"',word)\r\n",
        "            if len(match):\r\n",
        "              match = match[0]\r\n",
        "              #print(\"************start\",start)\r\n",
        "              if start:\r\n",
        "                ner_type = \"B-\"\r\n",
        "                start = 0\r\n",
        "              else:\r\n",
        "                ner_type = \"I-\"\r\n",
        "              if match == \"ORGANIZATION\":\r\n",
        "                ner_type += \"ORG\"\r\n",
        "              if match == \"LOCATION\":\r\n",
        "                ner_type += \"LOC\"\r\n",
        "              if match == \"PERSON\":\r\n",
        "                ner_type += \"PER\"\r\n",
        "\r\n",
        "              #print(\"TYPE:\",ner_type)\r\n",
        "            word = re.findall('>([A-ZÇĞİÖŞÜ]*[a-zçğıöşü]*)\\s*',word)\r\n",
        "            word =word[0]\r\n",
        "            #print(\"-----\",word,ner_type)\r\n",
        "            train_sent.append([word,ner_type])\r\n",
        "            count += 1\r\n",
        "          else:\r\n",
        "            if start:\r\n",
        "              ner_type = \"B-\"\r\n",
        "              start = 0\r\n",
        "            else:\r\n",
        "              ner_type = \"I-\"\r\n",
        "            if match == \"ORGANIZATION\":\r\n",
        "                ner_type += \"ORG\"\r\n",
        "            if match == \"LOCATION\":\r\n",
        "              ner_type += \"LOC\"\r\n",
        "            if match == \"PERSON\":\r\n",
        "              ner_type += \"PER\"\r\n",
        "            if word != \"<b_enamex\":\r\n",
        "              #print(\"Aradakiler\",word,ner_type)\r\n",
        "              train_sent.append([word,ner_type])\r\n",
        "              count += 1\r\n",
        "        else:\r\n",
        "          # if word.startswith(\"'\"):\r\n",
        "          #  if count >= 1:\r\n",
        "          #    train_sent[count-1][0] += word\r\n",
        "          #else:\r\n",
        "            train_sent.append([word,'O']) # they were indented\r\n",
        "            count += 1 # they were indented\r\n",
        "    \r\n",
        "        \r\n",
        "    all_sents.append(train_sent)\r\n",
        "      \r\n",
        "    #if count == 12:\r\n",
        "      #break    "
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLto3FqsDCMg"
      },
      "source": [
        "analysis_list = []\r\n",
        "analysis_words = []\r\n",
        "analysis_pairs  = []\r\n",
        "prev_line = 1\r\n",
        "sentences = []\r\n",
        "with open(\"/content/drive2/MyDrive/NLP/NE.ma.txt\",\"r\") as ner_ma: # read the morphological nalaysis file\r\n",
        "  count = 0\r\n",
        "  for line in ner_ma:\r\n",
        "    line = line.split()\r\n",
        "    analysis_list.append([line[0],line[1],line[2]])\r\n",
        "    analysis_words.append(line[1])\r\n",
        "    if (int(line[0]) > prev_line):\r\n",
        "      analysis_pairs.append(sentences)\r\n",
        "      sentences = []\r\n",
        "      sentences.append([line[1],line[2]])\r\n",
        "      prev_line = int(line[0])\r\n",
        "    else:\r\n",
        "      sentences.append([line[1],line[2]])\r\n",
        "  \r\n",
        "  analysis_pairs.append(sentences)"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6z_RTEfLXzW"
      },
      "source": [
        "X_train_all = []\r\n",
        "y_train_all = []\r\n",
        "for i in range(len(analysis_pairs)):\r\n",
        "  labels = []\r\n",
        "  sentence = []\r\n",
        "  count = 0\r\n",
        "  for j in analysis_pairs[i]:\r\n",
        "    word = j[0]\r\n",
        "    exist = 0\r\n",
        "    for pair in all_sents[i]:\r\n",
        "      if word == pair[0]:\r\n",
        "        exist = 1\r\n",
        "        labels.append(pair[1]) \r\n",
        "        #j.append(pair[1])\r\n",
        "        break\r\n",
        "    if not exist:\r\n",
        "      labels.append('O')\r\n",
        "    # add that word to the dictionary pair\r\n",
        "    feature_dict = {}\r\n",
        "    analysis = j[1]\r\n",
        "    first_index = analysis.find(\"+\") # get stem of the word\r\n",
        "    stem = analysis[:first_index]\r\n",
        "    feature_dict['Stem'] = stem\r\n",
        "    second_index = analysis.find(\"+\",first_index + 1)\r\n",
        "    POS = analysis[first_index +1:second_index]\r\n",
        "    feature_dict['POS'] = POS # get POS of the word\r\n",
        "    PROP = True if \"+Prop\" in analysis else False # check whether the analysis of the word includes '+Prop'\r\n",
        "    feature_dict['PROP'] = PROP\r\n",
        "    last_index = analysis.rfind(\"+\")\r\n",
        "    NCS = analysis[last_index+1:]\r\n",
        "    #print(NCS)\r\n",
        "    if NCS == \"Nom\" or NCS == \"Acc\" or NCS == \"Dat\" or NCS == \"Abl\" or NCS == \"Loc\" or NCS == \"Gen\" or NCS == \"Ins\" or NCS == \"Equ\":\r\n",
        "      feature_dict['NCS'] = NCS\r\n",
        "    else:\r\n",
        "      feature_dict['NCS'] = False # non nominal\r\n",
        "\r\n",
        "    INF = analysis[second_index+1:]\r\n",
        "    feature_dict['INF'] = INF\r\n",
        "    feature_dict['SS'] = True if count == 0 else False\r\n",
        "    feature_dict['OCS'] = True if word[0].isupper() else False\r\n",
        "    feature_dict['word.lower()'] = word.lower()\r\n",
        "    feature_dict['Person'] = False\r\n",
        "    feature_dict['Organization'] = False\r\n",
        "    feature_dict['Location'] = False\r\n",
        "    if word in persons.keys():    # checking if the word exists in my gazetters\r\n",
        "      feature_dict['Person'] = True\r\n",
        "    if word in locations.keys():\r\n",
        "      feature_dict['Location'] = True\r\n",
        "    if word in organizations.keys():\r\n",
        "      feature_dict['Organization'] = True\r\n",
        "    feature_dict['word.isupper()'] = word.isupper()\r\n",
        "    feature_dict['word.istitle()'] = word.istitle() # check whether onlt the first letter of the word is capitalized or not \r\n",
        "    feature_dict['word.isdigit()'] = word.isdigit()\r\n",
        "    feature_dict['word[-2:]'] = word[-2:] # last 2 letters\r\n",
        "    feature_dict['word[-3:]'] = word[-3:] # last 3 letters\r\n",
        "    sentence.append(feature_dict)\r\n",
        "    count += 1\r\n",
        "  X_train_all.append(sentence)\r\n",
        "  \r\n",
        "  y_train_all.append(labels)\r\n",
        "  #j.append('O')"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTExkyS8BXJK"
      },
      "source": [
        "# prepare data for cross validation described in the document\r\n",
        "new_X_train_all = []\r\n",
        "new_y_train_all = []\r\n",
        "for i in range(5):\r\n",
        "  count = i\r\n",
        "  while (count + 5 < len(X_train_all)):\r\n",
        "    new_X_train_all.append(X_train_all[count])\r\n",
        "    new_y_train_all.append(y_train_all[count])\r\n",
        "    count += 5"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ7uJ2-mvZiB",
        "outputId": "5d0d53d8-9217-4cbf-b6c4-89144cc78cf0"
      },
      "source": [
        "import numpy as np  # get the numpy arrays to use them in the cross validation\r\n",
        "X = np.array(new_X_train_all)\r\n",
        "y = np.array(new_y_train_all)"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ3CBCSfD1YQ"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "# build the models\r\n",
        "crf = sklearn_crfsuite.CRF(\r\n",
        "    algorithm='lbfgs',\r\n",
        "    c1=0.1,\r\n",
        "    c2=0.1,\r\n",
        "    max_iterations=100,\r\n",
        "    all_possible_transitions=False,\r\n",
        ")\r\n"
      ],
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpeK2dmAtp4n",
        "outputId": "ca8f231d-c254-4639-db85-408894a62340"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "\r\n",
        "results = {'B-LOC':[0,0,0],'B-ORG':[0,0,0],'B-PER':[0,0,0],'I-LOC':[0,0,0],'I-ORG':[0,0,0],'I-PER':[0,0,0]} # store the averages of f1-score, recall and precision\r\n",
        "count = 1\r\n",
        "for train_index, test_index in kf.split(X,y):\r\n",
        "  #print(test_index)\r\n",
        "  X_train, X_test = X[train_index], X[test_index]\r\n",
        "  y_train, y_test = y[train_index], y[test_index]\r\n",
        "  crf.fit(X_train,y_train)\r\n",
        "  y_pred = crf.predict(X_test)\r\n",
        "  labels = list(crf.classes_)\r\n",
        "  labels.remove('O')  # remove the 'O' tag\r\n",
        "  #labels\r\n",
        "  print(\"Fold number\",count)\r\n",
        "  count += 1\r\n",
        "  print(metrics.flat_classification_report(y_test, y_pred, labels = labels))\r\n",
        "  report_dict = metrics.flat_classification_report(y_test, y_pred, labels = labels,output_dict = True)\r\n",
        "  for key in results.keys():\r\n",
        "    results[key][0] += report_dict[key]['f1-score']\r\n",
        "    results[key][1] += report_dict[key]['precision']\r\n",
        "    results[key][2] += report_dict[key]['recall']\r\n",
        "  \r\n",
        "  print(\"*************************************\")\r\n",
        "\r\n",
        "for key in results.keys(): # compute the averages\r\n",
        "  results[key][0] /= 5\r\n",
        "  results[key][1] /= 5\r\n",
        "  results[key][2] /= 5\r\n",
        "\r\n",
        "#print average dict\r\n",
        "print(results)\r\n",
        "  #print(\"%s %s\" % (train, test))"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold number 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.93      0.84      0.88       489\n",
            "       B-PER       0.91      0.89      0.90       863\n",
            "       I-PER       0.89      0.87      0.88       333\n",
            "       B-ORG       0.87      0.79      0.83       494\n",
            "       I-ORG       0.80      0.77      0.78       331\n",
            "       I-LOC       0.81      0.58      0.68        67\n",
            "\n",
            "   micro avg       0.89      0.84      0.86      2577\n",
            "   macro avg       0.87      0.79      0.83      2577\n",
            "weighted avg       0.89      0.84      0.86      2577\n",
            "\n",
            "*************************************\n",
            "Fold number 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ORG       0.83      0.76      0.79       443\n",
            "       I-ORG       0.75      0.70      0.72       310\n",
            "       B-LOC       0.89      0.89      0.89       464\n",
            "       B-PER       0.93      0.87      0.90       886\n",
            "       I-PER       0.89      0.87      0.88       385\n",
            "       I-LOC       0.72      0.60      0.65        52\n",
            "\n",
            "   micro avg       0.87      0.83      0.85      2540\n",
            "   macro avg       0.83      0.78      0.81      2540\n",
            "weighted avg       0.87      0.83      0.85      2540\n",
            "\n",
            "*************************************\n",
            "Fold number 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ORG       0.88      0.81      0.84       446\n",
            "       I-ORG       0.82      0.75      0.78       298\n",
            "       B-LOC       0.91      0.89      0.90       505\n",
            "       B-PER       0.90      0.89      0.90       893\n",
            "       I-PER       0.88      0.84      0.86       391\n",
            "       I-LOC       0.83      0.54      0.65        65\n",
            "\n",
            "   micro avg       0.89      0.84      0.87      2598\n",
            "   macro avg       0.87      0.79      0.82      2598\n",
            "weighted avg       0.89      0.84      0.87      2598\n",
            "\n",
            "*************************************\n",
            "Fold number 4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ORG       0.85      0.75      0.80       510\n",
            "       I-ORG       0.76      0.66      0.71       316\n",
            "       B-LOC       0.88      0.85      0.86       520\n",
            "       B-PER       0.91      0.89      0.90       933\n",
            "       I-PER       0.80      0.90      0.85       365\n",
            "       I-LOC       0.76      0.61      0.68        79\n",
            "\n",
            "   micro avg       0.86      0.82      0.84      2723\n",
            "   macro avg       0.83      0.78      0.80      2723\n",
            "weighted avg       0.86      0.82      0.84      2723\n",
            "\n",
            "*************************************\n",
            "Fold number 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-ORG       0.88      0.79      0.83       466\n",
            "       I-ORG       0.77      0.71      0.74       289\n",
            "       B-LOC       0.88      0.85      0.86       517\n",
            "       B-PER       0.90      0.88      0.89       972\n",
            "       I-PER       0.86      0.87      0.87       407\n",
            "       I-LOC       0.75      0.61      0.67        74\n",
            "\n",
            "   micro avg       0.87      0.83      0.85      2725\n",
            "   macro avg       0.84      0.78      0.81      2725\n",
            "weighted avg       0.87      0.83      0.85      2725\n",
            "\n",
            "*************************************\n",
            "{'B-LOC': [0.8800085385300431, 0.8977496010984323, 0.8634470563019999], 'B-ORG': [0.81836010104605, 0.8612957518638659, 0.7796020594598094], 'B-PER': [0.8965731834338377, 0.9105432164982847, 0.883161252157992], 'I-LOC': [0.6665592370124332, 0.7757336655592469, 0.586481596334232], 'I-ORG': [0.7472496834491321, 0.7778781299143269, 0.719255857100497], 'I-PER': [0.8677078454564136, 0.8665423117398247, 0.8702696231070964]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhSqj17qxMsz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH4-8xGE5XAX",
        "outputId": "bbffee49-ca27-440d-8fa8-88f45d354ab5"
      },
      "source": [
        "# %%time\r\n",
        "# import scipy\r\n",
        "# from sklearn.metrics import fbeta_score, make_scorer\r\n",
        "# from sklearn.model_selection import RandomizedSearchCV\r\n",
        "# # define fixed parameters and parameters to search\r\n",
        "# crf = sklearn_crfsuite.CRF(\r\n",
        "#     algorithm='lbfgs',\r\n",
        "#     max_iterations=100,\r\n",
        "#     all_possible_transitions=True\r\n",
        "# )\r\n",
        "# params_space = {\r\n",
        "#     'c1': scipy.stats.expon(scale=0.5),\r\n",
        "#     'c2': scipy.stats.expon(scale=0.05),\r\n",
        "# }\r\n",
        "\r\n",
        "# # use the same metric for evaluation\r\n",
        "# f1_scorer = make_scorer(metrics.flat_f1_score,\r\n",
        "#                         average='weighted', labels=labels)\r\n",
        "\r\n",
        "# # search\r\n",
        "# rs = RandomizedSearchCV(crf, params_space,\r\n",
        "#                         cv=3,\r\n",
        "#                         verbose=1,\r\n",
        "#                         n_jobs=-1,\r\n",
        "#                         n_iter=50,\r\n",
        "#                         scoring=f1_scorer)\r\n",
        "# rs.fit(X_train, y_train)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  9.2min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 29.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 23min 47s, sys: 11.6 s, total: 23min 58s\n",
            "Wall time: 29min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}